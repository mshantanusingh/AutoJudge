# -*- coding: utf-8 -*-
"""ACM_rating_judge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjZ0y0RduL3YhdEdeqjctng9qapuu6wZ

# Importing Libraries
"""

!pip install lightgbm
!pip install catboost

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import unicodedata
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC, SVR
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression, RidgeClassifier
from sklearn.ensemble import GradientBoostingRegressor
from catboost import CatBoostClassifier
import lightgbm as lgb

import joblib

"""# Importing data"""

url="https://raw.githubusercontent.com/AREEG94FAHAD/TaskComplexityEval-24/main/problems_data.jsonl"
df=pd.read_json(url,lines=True)
df.head(5)

df.info()

"""# Data Preprocessing"""

df.drop(columns=['url','sample_io','title'],axis=1,inplace=True)

TEXT_COLS = ["description", "input_description", "output_description"]

for col in TEXT_COLS:
    df[col] = df[col].astype(str).str.strip()
    df.loc[df[col] == "", col] = np.nan
df.isna().sum()

df = df.dropna()

df.info()

def combine_text(row):
    return (
        "PROBLEM: " + str(row['description']) + " " +
        "INPUT_DESCRIPTION: " + str(row['input_description']) + " " +
        "OUTPUT_DESCRIPTION: " + str(row['output_description'])
    )

df['full_text'] = df.apply(combine_text, axis=1)

df.drop(columns=['description','input_description','output_description'],inplace=True)

LATEX_PATTERNS = [
    r'\$\$.*?\$\$',
    r'\$[^$]*\$',
    r'\\\(.+?\\\)',
    r'\\\[.+?\\\)'
]


def extract_latex(text):
    latex_blocks = []

    def repl(match):
        latex_blocks.append(match.group())
        return f" <LATEX_{len(latex_blocks)-1}> "

    for pattern in LATEX_PATTERNS:
        text = re.sub(pattern, repl, text)

    return text, latex_blocks



def clean_text(text):
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'\t', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()



def normalize_fractions(expr):
    pattern = r'\\frac\{([^{}]+)\}\{([^{}]+)\}'
    while re.search(pattern, expr):
        expr = re.sub(pattern, r'(\1)/(\2)', expr)
    return expr


LATEX_MAP = {
    r'\\leq': '<=',
    r'\\le': '<',
    r'\\ge': '>',
    r'\\geq': '>=',
    r'\\neq': '!=',
    r'\\times': '*',
    r'\\cdot': '*',
    r'\\dots': '...',
    r'\\ldots': '...',
    r'\\cdots': '...',
    r'\\\{': '{',
    r'\\\}': '}',
    r'\\,\s*': ''
}



def normalize_latex(expr):
    expr = expr.strip()
    if expr.startswith("$$") and expr.endswith("$$"):
        expr = expr[2:-2]
    elif expr.startswith("$") and expr.endswith("$"):
        expr = expr[1:-1]
    elif expr.startswith("\\(") and expr.endswith("\\)"):
        expr = expr[2:-2]
    elif expr.startswith("\\[") and expr.endswith("\\]"):
        expr = expr[2:-2]

    expr = normalize_fractions(expr)

    for k, v in LATEX_MAP.items():
        expr = re.sub(k, v, expr)

    return expr.strip()



def reinsert(text, latex_blocks, normalize=False):
    for i, block in enumerate(latex_blocks):
        if normalize:
            block = normalize_latex(block)
        text = text.replace(f"<LATEX_{i}>", block)
    return text



def preprocess(text):
    protected_text, latex_blocks = extract_latex(text)
    cleaned = clean_text(protected_text)
    semantic_version = reinsert(cleaned, latex_blocks, normalize=True)
    return semantic_version

df["clean_text"] = df["full_text"].apply(preprocess)
df.drop("full_text",axis=1,inplace=True)

df.head()

print(df["problem_class"].value_counts())
plt.hist(df['problem_class'])
plt.show()

print(df["problem_score"].describe())
plt.hist(df['problem_score'])
plt.show()

df.groupby("problem_class")["problem_score"].mean()

"""# Feature Engineering and Feature Extraction

## Text Embedding
"""

model=SentenceTransformer("all-MiniLM-L6-v2")
X_embed = model.encode(
    df["clean_text"].tolist(),
    batch_size=32,
    show_progress_bar=True,
    normalize_embeddings=True
)

"""## New features"""

df['text_length']=df['clean_text'].apply(len)

df.groupby("problem_class")["text_length"].mean()

def math_symbol_count(text):
    return sum(text.count(c) for c in "<>=^*/+-")

df['math_symbol_count']=df['clean_text'].apply(math_symbol_count)

df.groupby("problem_class")["math_symbol_count"].mean()

def num_count(text):
    return len(re.findall(r'\d+', text))
df['num_count']=df['clean_text'].apply(num_count)

df.groupby("problem_class")["num_count"].mean()

POWER_PATTERNS = [
    r'\b\w+\s*\^\s*\w+\b',
    r'\b\w+\s*\^\{\s*\w+\s*\}',
]

power_regex = re.compile("|".join(POWER_PATTERNS))

def has_power_notation(text):
    if not isinstance(text, str):
        return 0
    return int(bool(power_regex.search(text)))

df['has_power_notation']=df['clean_text'].apply(has_power_notation)

df.groupby("problem_class")["has_power_notation"].mean()

df.head()

"""# Data Preparation"""

feature_cols = ['has_power_notation','num_count','math_symbol_count','text_length']

df_features = df[feature_cols].values

X = np.hstack([X_embed, df_features])

list(set(df["problem_class"].values))

ordinal_order = ['easy', 'medium', 'hard']

ordinal_mapping = {label: idx for idx, label in enumerate(ordinal_order)}

df["problem_class_encoded"] = df["problem_class"].map(ordinal_mapping)
y_class = df["problem_class_encoded"]
y_score = df["problem_score"].values

X_train, X_test, y_class_train, y_class_test, y_score_train, y_score_test = train_test_split(
    X, y_class, y_score, test_size=0.2, random_state=42, stratify=y_class
)

scaler = StandardScaler()

scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Classification

## Logistic Regression
"""

print("Training LR...\n")

Logistic_Regression_model = LogisticRegression(
    class_weight="balanced",
    max_iter=5000,
    n_jobs=-1
)

Logistic_Regression_model.fit(X_train, y_class_train)

y_pred = Logistic_Regression_model.predict(X_test)

acc = accuracy_score(y_class_test, y_pred)
print(f"Logistic_Regression Accuracy: {acc:.4f}\n")
f1_macro = f1_score(y_class_test, y_pred, average="macro")
print(f"Logistic_Regression F1 : {f1_macro:.4f}\n")

cm = confusion_matrix(y_class_test, y_pred)

cm_df = pd.DataFrame(cm)

print("Confusion Matrix:")
print(cm_df)

train_class_pred = Logistic_Regression_model.predict(X_train)
acc = accuracy_score(train_class_pred, y_class_train)
print(f"Logistic Regression Train Accuracy: {acc:.4f}\n")

"""## RBF SVM Classifier"""

print("Training SVM...\n")

svm_model = SVC(kernel="rbf", class_weight="balanced",C=0.5, gamma=0.01 , random_state=42)
svm_model.fit(X_train, y_class_train)

y_pred = svm_model.predict(X_test)

acc = accuracy_score(y_class_test, y_pred)
print(f"SVM Accuracy: {acc:.4f}\n")
f1_macro = f1_score(y_class_test, y_pred, average="macro")
print(f"SVM F1 : {f1_macro:.4f}\n")

cm = confusion_matrix(y_class_test, y_pred)

cm_df = pd.DataFrame(cm)

print("Confusion Matrix:")
print(cm_df)

train_class_pred = svm_model.predict(X_train)

acc = accuracy_score(train_class_pred, y_class_train)
print(f"SVM Train Accuracy: {acc:.4f}\n")

"""## LightGBM"""

lgb_clf = lgb.LGBMClassifier(
    objective="multiclass",
    num_class=3,
    n_estimators=300,
    learning_rate=0.03,
    max_depth=5,
    num_leaves=10,
    min_data_in_leaf=50,
    lambda_l1=1.0,
    lambda_l2=5.0,
    subsample=0.7,
    colsample_bytree=0.7,
    random_state=42,
    verbose=-1
)



lgb_clf.fit(X_train, y_class_train)

y_pred = lgb_clf.predict(X_test)

acc = accuracy_score(y_class_test, y_pred)
print(f"LightGBM Accuracy: {acc:.4f}\n")
f1_macro = f1_score(y_class_test, y_pred, average="macro")
print(f"LightGBM F1 : {f1_macro:.4f}\n")

cm = confusion_matrix(y_class_test, y_pred)

cm_df = pd.DataFrame(cm)

print("Confusion Matrix:")
print(cm_df)

train_class_pred = lgb_clf.predict(X_train)
acc = accuracy_score(train_class_pred, y_class_train)
print(f"LightGBM Train Accuracy: {acc:.4f}\n")

"""## CatBoost Classifier"""

cat_clf = CatBoostClassifier(
    iterations=300,
    learning_rate=0.05,
    depth=6,
    loss_function='MultiClass',
    random_seed=42,
    verbose=50,
)

cat_clf.fit(
    X_train, y_class_train
)

y_pred = cat_clf.predict(X_test)

acc = accuracy_score(y_class_test, y_pred)
print(f"CatBoost Accuracy: {acc:.4f}\n")
f1_macro = f1_score(y_class_test, y_pred, average="macro")
print(f"CatBoost F1 : {f1_macro:.4f}\n")

cm = confusion_matrix(y_class_test, y_pred)

cm_df = pd.DataFrame(cm)

print("Confusion Matrix:")
print(cm_df)

train_class_pred = cat_clf.predict(X_train)
acc = accuracy_score(train_class_pred, y_class_train)
print(f"CatBoost Train Accuracy: {acc:.4f}\n")

"""# Regression

## Ridge Regressor
"""

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_score_train)
y_pred = ridge.predict(X_test)


mae = mean_absolute_error(y_score_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_score_test, y_pred))

print("Ridge Regressor")
print("MAE :", mae)
print("RMSE:", rmse)

train_pred = ridge.predict(X_train)
print("Train MAE:", mean_absolute_error(y_score_train, train_pred))

df.groupby("problem_class")["problem_score"].std()

plt.scatter(y_score_test, y_pred)
plt.plot([0,10],[0,10])
plt.xlabel("True Score")
plt.ylabel("Predicted Score")
plt.show()

plt.scatter(y_score_train, train_pred)
plt.plot([0,10],[0,10])
plt.xlabel("True Score")
plt.ylabel("Predicted Score")
plt.show()

"""## Gradient Boosting Regressor"""

gbr = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=3)
gbr.fit(X_train, y_score_train)
y_pred= gbr.predict(X_test)

mae = mean_absolute_error(y_score_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_score_test, y_pred))

print("Gradient Boost Regressor")
print("MAE :", mae)
print("RMSE:", rmse)

train_pred = gbr.predict(X_train)
print("Train MAE:", mean_absolute_error(y_score_train, train_pred))

df.groupby("problem_class")["problem_score"].std()

plt.scatter(y_score_test, y_pred)
plt.plot([0,10],[0,10])
plt.xlabel("True Score")
plt.ylabel("Predicted Score")
plt.show()

plt.scatter(y_score_train, train_pred)
plt.plot([0,10],[0,10])
plt.xlabel("True Score")
plt.ylabel("Predicted Score")
plt.show()

"""## LightGBM regressor"""

lgb_reg = lgb.LGBMRegressor(
    objective="regression",
    n_estimators=300,
    learning_rate=0.03,
    num_leaves=15,
    max_depth=6,
    min_data_in_leaf=50,
    lambda_l1=1.0,
    lambda_l2=5.0,
    subsample=0.7,
    colsample_bytree=0.7,
    random_state=42,
    verbose=-1
)


lgb_reg.fit(X_train, y_score_train)
y_pred = lgb_reg.predict(X_test)

mae = mean_absolute_error(y_score_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_score_test, y_pred))

print("LightGBM Regressor")
print("MAE :", mae)
print("RMSE:", rmse)

train_pred = lgb_reg.predict(X_train)
print("Train MAE:", mean_absolute_error(y_score_train, train_pred))

df.groupby("problem_class")["problem_score"].std()

plt.scatter(y_score_test, y_pred)
plt.plot([0,10],[0,10])
plt.xlabel("True Score")
plt.ylabel("Predicted Score")
plt.show()

plt.scatter(y_score_train, train_pred)
plt.plot([0,10],[0,10])
plt.xlabel("True Score")
plt.ylabel("Predicted Score")
plt.show()

"""# Saving the Model"""

best_model_clf = lgb_clf
best_model_reg = gbr

joblib.dump(best_model_clf, 'clf_model.pkl')
joblib.dump(best_model_reg, 'reg_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

from google.colab import files
files.download('clf_model.pkl')
files.download('reg_model.pkl')
files.download('scaler.pkl')